# ============================================================================
# SelcukAiAssistant Backend Configuration
# ============================================================================
# Copy this file to .env and adjust values as needed
# ============================================================================

# ----------------------------------------------------------------------------
# Server Configuration
# ----------------------------------------------------------------------------
# Host to bind the server to
# - Use 127.0.0.1 for localhost only (recommended for development)
# - Use 0.0.0.0 to allow external connections
HOST=127.0.0.1

# Port number for the server (1-65535)
PORT=8000

# ----------------------------------------------------------------------------
# CORS Configuration
# ----------------------------------------------------------------------------
# Comma-separated list of allowed origins for CORS
# - Use * to allow all origins (no credentials are allowed when using *)
# - In production, specify exact URLs: http://localhost:3000,https://app.example.com
# - For Flutter web dev, add the exact http://localhost:<port> shown by flutter run
ALLOWED_ORIGINS=http://localhost:8000,http://127.0.0.1:8000

# Require explicit ALLOWED_ORIGINS (disables localhost fallback)
ALLOWED_ORIGINS_STRICT=false

# ----------------------------------------------------------------------------
# Ollama Configuration
# ----------------------------------------------------------------------------
# Base URL of the Ollama service
OLLAMA_BASE_URL=http://localhost:11434

# Model name to use (without :latest tag, it will be handled automatically)
# For this project, use: selcuk_ai_assistant
# Default fallback: llama3.1
OLLAMA_MODEL=selcuk_ai_assistant

# Request timeout in seconds
OLLAMA_TIMEOUT=120

# Maximum number of retry attempts for failed requests
OLLAMA_MAX_RETRIES=3

# Delay between retries in seconds (exponential backoff)
OLLAMA_RETRY_DELAY=1.0

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# - DEBUG: Detailed information for debugging
# - INFO: General informational messages (recommended)
# - WARNING: Warning messages
# - ERROR: Error messages only
LOG_LEVEL=INFO

# ----------------------------------------------------------------------------
# RAG (Retrieval-Augmented Generation) Configuration
# ----------------------------------------------------------------------------
# Enable RAG for context-aware responses (true/false)
# Set to false until vector database is set up
RAG_ENABLED=false

# Path to ChromaDB vector database storage
# Example: ./data/chromadb or /var/lib/selcuk-ai/vectordb
RAG_VECTOR_DB_PATH=./data/chromadb

# Collection name in the vector database
RAG_COLLECTION_NAME=selcuk_documents

# Document chunk size for RAG ingestion (in characters)
RAG_CHUNK_SIZE=500

# Overlap between document chunks (in characters)
RAG_CHUNK_OVERLAP=50

# ----------------------------------------------------------------------------
# Appwrite Integration (Future)
# ----------------------------------------------------------------------------
# Endpoint for Appwrite API
APPWRITE_ENDPOINT=https://cloud.appwrite.io/v1

# Project ID from Appwrite console
APPWRITE_PROJECT_ID=

# API Key with appropriate scopes (databases.read, etc.)
APPWRITE_API_KEY=
# Database ID in Appwrite
APPWRITE_DATABASE_ID=
# Collection ID for storing chat logs
APPWRITE_COLLECTION_ID=
# ----------------------------------------------------------------------------
# Model Routing
# ----------------------------------------------------------------------------
# Default backend to use: ollama or huggingface
MODEL_BACKEND=ollama

# Optional model aliases: alias=provider:model_id
# Example:
# MODEL_ALIASES=ollama_default=ollama:selcuk_ai_assistant,hf_qwen_1_5b=huggingface:Qwen/Qwen2.5-1.5B-Instruct
MODEL_ALIASES=

# ----------------------------------------------------------------------------
# Hugging Face Transformers
# ----------------------------------------------------------------------------
# Default HF model to use when MODEL_BACKEND=huggingface
HF_MODEL_NAME=Qwen/Qwen2.5-1.5B-Instruct

# Load in 4-bit (requires bitsandbytes). Recommended for RTX 3060 6GB.
HF_LOAD_IN_4BIT=true

# Device: auto, cuda, cpu
HF_DEVICE=cuda

# Dtype: float16, bfloat16, float32
HF_DTYPE=bfloat16

# Attention implementation: flash_attention_2, sdpa, eager
HF_ATTENTION_IMPL=sdpa

# ----------------------------------------------------------------------------
# Guardrails
# ----------------------------------------------------------------------------
# Max context and output tokens
MAX_CONTEXT_TOKENS=4096
MAX_OUTPUT_TOKENS=512

# Request timeout (seconds)
REQUEST_TIMEOUT=120
# ============================================================================
# End of Configuration
# ============================================================================
# ----------------------------------------------------------------------------
# Rate Limiting (Future Implementation)
# ----------------------------------------------------------------------------
# These are placeholders for future rate limiting implementation
# RATE_LIMIT_ENABLED=false
# RATE_LIMIT_REQUESTS_PER_MINUTE=60
# RATE_LIMIT_REQUESTS_PER_HOUR=1000
# ----------------------------------------------------------------------------
# Security (Future Implementation)
# ----------------------------------------------------------------------------
# API Key for securing backend endpoints
# API_KEY=
# ----------------------------------------------------------------------------
# End of File
# ----------------------------------------------------------------------------
